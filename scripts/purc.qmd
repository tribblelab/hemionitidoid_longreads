---
title: "running purc"
output: github_document
date: 2025-10-23
---

# goal: run PURC 2.0 on Hemionitidoid longread (PacBio) data on the HYAK cluster.

PURC: 
    - uses model to remove PCR + sequencing errors, infers amplicon sequence variants (ASVs)
      done with DADA2
    - treats the conensus sequence of each cluster as an OTU
    - PacBio long reads are critical here:
        - short reads may be indistinguishable from each (sub)genome copy 
        - basically they make a long circular strand of DNA, which can be read many times

## preparing files

to do list:
[] send pics of weird alignments to slack
[] make a dummy run -->
  [] new config
  [] new bash script
  [] new reference file
  [] new map

### sequence data

to start, we need to convert our .bam files from PacBio to .fastq.
(.fasta is also acceptable, but can't do ASV stuff with .fasta)

in the `/mmfs1/gscratch/tribblelab/hemionitidoid_longreads` dir:

```{bash}
salloc --partition=cpu-g2 --cpus-per-task=1 --mem=10G --time=5:00:00
conda activate pacbio
bam2fastq -o pentpoolA m64044_221017_183110.subreads.demux.pentpoolA--pentpoolA.bam
bam2fastq -o pentpoolB m64044_221017_183110.subreads.demux.pentpoolB--pentpoolB.bam
```

While the command only uses the .bam file as an argument, you will also need the .bam.pbi index file 
for this command to run.

Note: I ran this initially on an interactive node, but it timed out.
Instead, ran as a slurm job via [`scripts/bamtofastq.slurm`](`bamtofastq.slurm`)

The bam2fastq conversion results in a .gz compressed file. I uncompressed using
via [`scripts/gzdecompression.slurm`](`gzdecompression.slurm`)

but my files are too huge, I'll have to split into batches of 2 million reads each:
```{bash}
input="pentpoolA.fastq"
parent="poolafastq"
mkdir -p "$parent"

# Split FASTQ into batches of 2M reads (8M lines)
awk -v parent="$parent" '
BEGIN { batch_size = 8000000 }  # 2 million reads Ã— 4 lines
{
    file_num = int((NR-1)/batch_size) + 1
    folder = sprintf("%s/batch_%03d", parent, file_num)
    fname = folder "/reads.fastq"

    # Create folder at the first line of the batch
    if (NR % batch_size == 1) {
        system("mkdir -p " folder)
    }

    print >> fname
}
' "$input"

# Only delete input if AWK succeeded
if [ $? -eq 0 ]; then
    echo "Splitting complete. Deleting input FASTQ..."
    rm -f "$input"
else
    echo "Error occurred; input file was NOT deleted."
fi
```

fixit: will eventually have to do for pool B.

### barcode file

needs to be in .fasta format, with the ID as barcode name, and barcode in 5'-3' direction.
there is a dual barcode function, in which their names will be `XXX1` (F) and `XXX2` (R)
got the data from `Wefferling_barcoded_primers_2019.xlsx` file, just pasted it all in one file
and formatted correctly. 

### ref seq file

must be in .fasta format. headers must be of this format:
`locus=ApP/group=A/ref_taxon=Cystopteris_bulbifera`
where each field is separated by a forward slash.
cannot exceed 50 characters (BLAST search will fail)

note: I have the `efetch` command available through setting up `edirect` locally a while ago... I'm assuming I set up edirect using [these directions](https://www.nlm.nih.gov/dataguide/edirect/install.html) for download

code to scrape reference sequences from NCBI from a CSV that contains the codes for each sequence:
(note: I manually copy + pasted the sequences if they came from full plastomes)
```{julia}
using DataFrames, CSV
ncbidata = DataFrame(CSV.File("input/references.csv",stringtype=String)) #load df from CSV
outputfile = "input/reference.fasta"
open(outputfile, "w") do io
  for row in eachrow(ncbidata)
      spname = row.shortname
      group = row.group_abbrev
      gene = row.gene
      header = ">locus=" * gene * "/group=" * group * "/ref_taxon=" * spname
      #write header to .txt file
      println(io, header)
      id = row.NCBIcode
      fullplastome = row.fullplastome
      if !ismissing(id) && (!ismissing(fullplastome) && fullplastome == false)
            tmpfile = tempname()
            run(pipeline(`efetch -db nuccore -id $id -format fasta`, stdout = tmpfile))
            #append the sequence (skipping the efetch header line)
            for line in eachline(tmpfile)
                if !startswith(line, ">")
                    println(io, line)
                end
            end
            rm(tmpfile; force = true)
        #write empty line to file
        println(io)
      end
      #write file out
  end
end
```

fixits from this file:
[X] ones I have to grab from plastome
[] 2 missing pentagramma genes --> build seqs from Keir files
[] line breaks for seqs?
[] check alignments in geneious to make sure sequences are mostly of similar length/overlap

problems:
  1. appefp-c: poor alignment, check the reverse complements?
  2. cry2: replacing Dory (available on NCBI) for Gaga, since our Dory samples didn't amplify cry2 at all. but the Pent cry2 ref sequence is proving a small problem.. from the sanger data, none of the forward primers seemed to work well. and reverse primers give maybe ~500bp/2,000bp, which is better than nothing.
  3. gapcpsh: big deletions / gaps between pent/noth vs. myri/parahemionitis ?
  7. pgic: beautiful alignment, just need to add in penta
  10. sqd1: bad alignment

fixed:
  4. ibr3: original myriopteris seq was HUGE relative to others, I pulled a smaller one (M. aurea, MN876455.1) it aligned much better.
  8. psbm: notholaena- bad annotation on full plastome on ncbi??? I manually found correct seq.
  should maybe go see what annotation for seq I pulled is???

no problems:
  5. infa: amazing, no notes
  6. matk: amazing, no notes
  9. rpl2: amazing, no notes

### map files

tsv file. indicating which barcodes/groups go with each sample.
need a file for *each locus*!
I made an excel sheet `pentagramma_sampling_lauren.xlsx` that cleaned up
the formatting and with some sorting / hiding of columns, I was able to paste what I 
needed to make each individual file. (ok, could be more efficient doing this maybe if I wrote
some code, but wasn't terrible by hand)

### config file

## running job

```{bash}
sbatch --array=1-2 scripts/purc_A.slurm
sbatch scripts/purc_A.slurm
```