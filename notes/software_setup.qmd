---
title: "software install notes"
output: github_document
date: 2025-10-22
---

# Software set-up notes for analyses

## lilNASx data storage

Location of data: `/var/services/NetBackup/hemionitid_PacBio/data`

So apparently, all the *.tar.partaa, *.tar.partab, etc. files are separated parts of the same tar.
let's `cat` them back together then untar.

```{bash}
cat megaPent_Cell_1.tar.parta* | tar -xvf -
```

(done on the lab desktop, so the command doesn't time out. 
as of 10-27-25, `tmux` or `screen` commands aren't available)

After untar'ing the files look like this:

> barcodes.fasta
> m64044_221017_183110.baz2bam_1.log
> m64044_221017_183110.scraps.bam
> m64044_221017_183110.scraps.bam.pbi
> m64044_221017_183110.sts.xml
> m64044_221017_183110.subreads
> m64044_221017_183110.subreads.bam
> m64044_221017_183110.subreads.bam.pbi
> m64044_221017_183110.subreads.demux.json
> m64044_221017_183110.subreads.demux.lima.counts
> m64044_221017_183110.subreads.demux.lima.guess
> m64044_221017_183110.subreads.demux.lima.report
> m64044_221017_183110.subreads.demux.pentpoolA--pentpoolA.bam
> m64044_221017_183110.subreads.demux.pentpoolA--pentpoolA.bam.pbi
> m64044_221017_183110.subreads.demux.pentpoolA--pentpoolA.subreadset.xml
> m64044_221017_183110.subreads.demux.pentpoolB--pentpoolB.bam
> m64044_221017_183110.subreads.demux.pentpoolB--pentpoolB.bam.pbi
> m64044_221017_183110.subreads.demux.pentpoolB--pentpoolB.subreadset.xml
> m64044_221017_183110.subreads.demux.subreadset.xml
> m64044_221017_183110.subreadset.xml
> m64044_221017_183110.transferdone

For more info on the file formats produced by PacBio,
read [here](https://pacbio.gs.washington.edu/documents/Raw_Data_files.pdf) or [here](https://pacbiofileformats.readthedocs.io/en/13.0/BAM.html).

Transferring the two .bam + .bam.pbi datafiles to HYAK:
```{bash}
scp m64044_221017_183110.subreads.demux.pentpoolA--pentpoolA.bam laufran@klone.hyak.uw.edu:/mmfs1/gscratch/tribblelab/hemionitidoid_longreads
scp m64044_221017_183110.subreads.demux.pentpoolA--pentpoolA.bam.pbi laufran@klone.hyak.uw.edu:/mmfs1/gscratch/tribblelab/hemionitidoid_longreads
scp m64044_221017_183110.subreads.demux.pentpoolB--pentpoolB.bam laufran@klone.hyak.uw.edu:/mmfs1/gscratch/tribblelab/hemionitidoid_longreads
scp m64044_221017_183110.subreads.demux.pentpoolB--pentpoolB.bam.pbi laufran@klone.hyak.uw.edu:/mmfs1/gscratch/tribblelab/hemionitidoid_longreads
```

(as of 10-22-25, Globus isn't set up on the NAS)

## Getting used to HYAK

- Command to `ssh` in: `ssh laufran@klone.hyak.uw.edu`
- Path to data storage for this project: `/mmfs1/gscratch/tribblelab/hemionitidoid_longreads`
    - in the `/mmfs1/gscratch/tribblelab/` directory, we should be able to store 1TB of data 
    ("Your lab gets 1 TB per slice that your group has contributed to klone, 
    which includes HPC (CPU-only) and GPU slices." 
    [source](https://hyak.uw.edu/docs/storage/gscratch))
- In the above directory, git clone the PURC 2.0 bitbucket repo: `git clone https://bitbucket.org/peter_schafran/purc.git`

### Finding our allocation

```{bash}
[laufran@klone-login03 basics]$ hyakalloc
```

      Account resources available to user: laufran       
╭────────────┬───────────┬──────┬────────┬──────┬───────╮
│    Account │ Partition │ CPUs │ Memory │ GPUs │       │
├────────────┼───────────┼──────┼────────┼──────┼───────┤
│ tribblelab │    cpu-g2 │   64 │   490G │    0 │ TOTAL │
│            │           │    0 │     0G │    0 │ USED  │
│            │           │   64 │   490G │    0 │ FREE  │
╰────────────┴───────────┴──────┴────────┴──────┴───────╯
 Checkpoint Resources  
╭───────┬──────┬──────╮
│       │ CPUs │ GPUs │
├───────┼──────┼──────┤
│ Idle: │ 3062 │   96 │
╰───────┴──────┴──────╯

great, our partition is for `cpu-g2`.

to test out getting an interactive node with 1 CPU, 10GB memory max, 2 hours
`salloc --partition=cpu-g2 --cpus-per-task=1 --mem=10G --time=2:00:00`

to check on the amount of storage used, either in personal home directory 
or in the lab directory, do: `hyakstorage`. 
If you've recently deleted/added files, it updates every hour so might not be accurate. 
Storage should hopefully be less of a concern now with the conda module available.

### Setting up conda envs for PURC + PacBio

notably, as of December 2025, there's a conda module now available. I've since deleted 
my prior miniconda set up in favor of this module, which should take up less storage
and be less buggy. 

to delete prior miniconda setup:
```{bash}
cd /mmfs1/gscratch/tribblelab/laufran/miniconda3
./uninstall.sh
```

additionally, have to go to `~/.bashrc` and delete miniconda set up lines.

```{bash}
salloc --partition=cpu-g2 --cpus-per-task=1 --mem=10G --time=2:00:00
module load conda
```

now go to `~/.condarc`
and add these lines:
```
envs_dirs:
  - /gscratch/tribblelab/laufran/conda/envs
pkgs_dirs:
  - /gscratch/tribblelab/laufran/conda/pkgs
```

```{bash}
cd /mmfs1/gscratch/tribblelab/laufran
# this will take a minute, it's downloading all the dependencies
conda env create -n purc --file purc/purc_linux.yaml 
```

There is a command to convert .bam --> .fasta,
using tools supported by PacBio, as set up by a conda env.
However, they require Python 2.7, not 3.X+ (as above)
```{bash}
conda create -n pacbio python=2.7
conda activate pacbio
conda install -c bioconda pbtk
```